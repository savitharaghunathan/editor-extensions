# This is a sample settings file generated by the extension with a few common providers
# and models defined.  To return the file back to default, delete it and restart vscode.
---
environment:
  ALWAYS_APPLIED_KEY: "envvar to be set regardless of which model is active"
  # CA_BUNDLE: "" # optional
  # ALLOW_INSECURE: "false" # optional

# This is a collection of model configurations that may be used.  The `&active`
# anchor is used to reference the model to the active node below.  The extension will
# use the active node for configuration the kai-rpc-server.
models:
  OpenAI: &active
    environment:
      OPENAI_API_KEY: "" # Required
    provider: ChatOpenAI
    args:
      model: gpt-4o # Required

      ## Following keys are optional & only exist for documentation purposes
      # user: <string>
      # temperature: <number>
      # maxTokens: <number>
      # timeout: <number>
      # maxRetries: <number>
      # configuration:
      #   baseURL: <string>
      #   project: <string>
      #   organization: <string>

  AzureChatOpenAI:
    environment:
      AZURE_OPENAI_API_KEY: "" # Required
    provider: AzureChatOpenAI
    args:
      azureOpenAIApiDeploymentName: "" # Required
      azureOpenAIApiVersion: "" # Required

      ## Following keys are optional & only exist for documentation purposes
      # azureOpenAIApiInstanceName: <string>
      # azureOpenAIBasePath: <string>
      # azureOpenAIEndpoint: <string>
      # azureOpenAIDeploymentName: <string>
      # openAIApiVersion: <string>
      # openAIBasePath: <string>
      # deploymentName: <string>
      # temperature: <number>
      # maxTokens: <number>
      # timeout: <number>
      # maxRetries: <number>
      # modelKwargs: <object>

  AmazonBedrock:
    environment:
      AWS_ACCESS_KEY_ID: "" # Required if a global ~/.aws/credentials file is not present
      AWS_SECRET_ACCESS_KEY: "" # Required if a global ~/.aws/credentials file is not present
      AWS_DEFAULT_REGION: "" # Required
    provider: ChatBedrock
    args:
      model: meta.llama3-70b-instruct-v1:0 # Required

      ## Following keys are optional & only exist for documentation purposes
      # temperature: <number>
      # additionalModelRequestFields: <object>
      # configFilepath: <string>

  DeepSeek:
    environment:
      DEEPSEEK_API_KEY: "" # Required
    provider: ChatDeepSeek
    args:
      model: deepseek-chat # Required

      ## Following keys are optional & only exist for documentation purposes
      # temperature: <number>
      # maxTokens: <number>
      # timeout: <number>
      # maxRetries: <number>
      # modelKwargs: <object>

  GoogleGenAI:
    environment:
      GOOGLE_API_KEY: "" # Required
    provider: ChatGoogleGenerativeAI
    args:
      model: gemini-2.5-pro # Required

      ## Following keys are optional & only exist for documentation purposes
      # apiVersion: <string>
      # baseUrl: <string>
      # temperature: <number>

  # If running locally https://ollama.com/, get the model name via `ollama list`
  ChatOllama:
    provider: ChatOllama
    args:
      model: your-model-here # Required

      ## Following keys are optional & only exist for documentation purposes
      # temperature: <number>
      # numPredict: <number>

  JustAnExample:
    environment:
      ANY_KEY_1: "any environment variable needed for this model provider"
      ANY_KEY_2: "any environment variable needed for this model provider"

    provider: "provider-string"
    args:
      anyArgumentName1: "argument one"
      anyArgumentName2: "argument two"
      any-argument-name-3: "argument three"

    template: "template string" # optional
    llamaHeader: "header string" # optional
    llmRetries: 5 # optional number, defaults to 5
    llmRetryDelay: 10.0 # optional float, default is 10.0

# This is the node used for configuring the server.  A simple anchor/reference
# pair is an easy way to to select a configuration.  To change configs, move the
# `&active` anchor to the desired block and restart the server.
active: *active
